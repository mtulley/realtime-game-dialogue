@book{barton2008dungeonsanddesktops,
  title={Dungeons \& Desktops: The History of Computer Role-Playing Games},
  author={Barton, Matt},
  year={2008},
  publisher={AK Peters Ltd},
  address={Wellesley, MA}
}

@misc{howard2018universallanguagemodelfinetuning,
      title={Universal Language Model Fine-tuning for Text Classification}, 
      author={Jeremy Howard and Sebastian Ruder},
      year={2018},
      eprint={1801.06146},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1801.06146}, 
}

@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}

@inproceedings{akoury_framework_2023,
	address = {Singapore},
	title = {A {Framework} for {Exploring} {Player} {Perceptions} of {LLM}-{Generated} {Dialogue} in {Commercial} {Video} {Games}},
	url = {https://aclanthology.org/2023.findings-emnlp.151/},
	doi = {10.18653/v1/2023.findings-emnlp.151},
	abstract = {The growing capabilities of large language models (LLMs) have inspired recent efforts to integrate LLM-generated dialogue into video games. However, evaluation remains a major challenge: how do we assess the player experience in a commercial game augmented with LLM-generated dialogue? To explore this question, we introduce a dynamic evaluation framework for the dialogue management systems that govern the task-oriented dialogue often found in roleplaying video games. We first extract dialogue from the widely-acclaimed role-playing game *Disco Elysium: The Final Cut*, which contains 1.1M words of dialogue spread across a complex graph of utterances where node reachability depends on game state (e.g., whether a certain item is held). Using this dataset, we have GPT-4 perform *dialogue infilling* to generate grounded utterances based on game state represented via code. In a statistically robust study of 28 players recruited from the r/DiscoyElysium subreddit, the LLM outputs are evaluated against the game designers' writing via both preference judgments and free-form feedback using a web interface that recreates the game`s core conversation functionality. Overall, the game designers' prose is significantly preferred to GPT-4 generations, with participants citing reasons such as improved logical flow and grounding with the game state. To spur more principled future research in this area, we release our web interface and tools to enable researchers to build upon our work. https://pl.aiwright.dev},
	urldate = {2025-03-01},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Akoury, Nader and Yang, Qian and Iyyer, Mohit},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {2295--2311},
}

@article{csepregi_effect_nodate,
	title = {The {Effect} of {Context}-aware {LLM}-based {NPC} {Conversations} on {Player} {Engagement} in {Role}-playing {Video} {Games}},
	abstract = {This research investigates the impact of integrating context-aware large language model-based non-player character (NPC) conversations on player engagement in role-playing games. By utilizing the capabilities of context-aware dialogues generated by language models, the study aims to enhance player engagement with context-aware NPCs. Through an experimental study (n=21), measuring the player engagement and perceived conversation quality, the results demonstrate that context-aware LLM-based NPC conversations have high potential in increasing player engagement in RPGs. Design guidelines for implementing context-aware NPC interactions are identified, while acknowledging the limitations of the study. Further research is recommended to explore other aspects of the topic.},
	language = {en},
	author = {Csepregi, Lajos Matyas},
}

@inproceedings{cox_conversational_2024,
	address = {Cham},
	title = {Conversational {Interactions} with {NPCs} in {LLM}-{Driven} {Gaming}: {Guidelines} from a {Content} {Analysis} of {Player} {Feedback}},
	isbn = {978-3-031-54975-5},
	shorttitle = {Conversational {Interactions} with {NPCs} in {LLM}-{Driven} {Gaming}},
	doi = {10.1007/978-3-031-54975-5_10},
	abstract = {The growing capability and availability of large language models (LLMs) have led to their adoption in a number of domains. One application domain that could prove fruitful is to video games, where LLMs could be used to provide conversational responses from non-playable characters (NPCs) that are more dynamic and diverse. Additionally, LLMs could allow players the autonomy to converse in open-ended conversations potentially improving player immersion and agency. However, due to their recent commercial popularity, the consequences (both negative and positive) of using LLMs in video games from a player’s perspective is currently unclear. On from this, we analyse player feedback to the use of LLM-driven NPC responses in a commercially available video game. We discuss findings and implications, and generate guidelines for designers incorporating LLMs into NPC dialogue.},
	language = {en},
	booktitle = {Chatbot {Research} and {Design}},
	publisher = {Springer Nature Switzerland},
	author = {Cox, Samuel Rhys and Ooi, Wei Tsang},
	editor = {Følstad, Asbjørn and Araujo, Theo and Papadopoulos, Symeon and Law, Effie L.-C. and Luger, Ewa and Goodwin, Morten and Hobert, Sebastian and Brandtzaeg, Petter Bae},
	year = {2024},
	pages = {167--184},
}

@inproceedings{christiansen_exploring_2024,
	address = {New York, NY, USA},
	series = {{VRST} '24},
	title = {Exploring {Presence} in {Interactions} with {LLM}-{Driven} {NPCs}: {A} {Comparative} {Study} of {Speech} {Recognition} and {Dialogue} {Options}},
	isbn = {979-8-4007-0535-9},
	shorttitle = {Exploring {Presence} in {Interactions} with {LLM}-{Driven} {NPCs}},
	url = {https://dl.acm.org/doi/10.1145/3641825.3687716},
	doi = {10.1145/3641825.3687716},
	abstract = {Combining modern technologies like large-language models (LLMs), speech-to-text, and text-to-speech can enhance immersion in virtual reality (VR) environments. However, challenges exist in effectively implementing LLMs and educating users. This paper explores implementing LLM-powered virtual social actors and facilitating user communication. We developed a murder mystery game where users interact with LLM-based non-playable characters (NPCs) through interrogation, clue-gathering, and exploration. Two versions were tested: one using speech recognition and another with traditional dialog boxes. While both provided similar social presence, users felt more immersed with speech recognition but found it overwhelming, while the dialog version was more challenging. Slow NPC response times were a source of frustration, highlighting the need for faster generation or better masking for a seamless experience.},
	urldate = {2025-02-28},
	booktitle = {Proceedings of the 30th {ACM} {Symposium} on {Virtual} {Reality} {Software} and {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Christiansen, Frederik Roland and Hollensberg, Linus Nørgaard and Jensen, Niko Bach and Julsgaard, Kristian and Jespersen, Kristian Nyborg and Nikolov, Ivan},
	month = oct,
	year = {2024},
	pages = {1--11},
}

@inproceedings{marincioni_effect_2024,
	title = {The {Effect} of {LLM}-{Based} {NPC} {Emotional} {States} on {Player} {Emotions}: {An} {Analysis} of {Interactive} {Game} {Play}},
	shorttitle = {The {Effect} of {LLM}-{Based} {NPC} {Emotional} {States} on {Player} {Emotions}},
	url = {https://ieeexplore.ieee.org/abstract/document/10645631},
	doi = {10.1109/CoG60054.2024.10645631},
	abstract = {This research study explores the emotional responses evoked during game play and interactions with non-player characters (NPCs) in a mystery-solving game. Structured around three phases-introduction, collaboration, and feedback-, the game uses large language models (LLMs) to simulate varying emotional states in NPCs, from neutrality to expressions of anger, joy, and more. Players’ emotional responses to the game play and interaction with the NPCs are captured through the dialogue they input. Language models are used to extract emotion scores from these in-game conversations. This study aims to enhance our understanding of emotional dynamics within gaming environments, in order to further inform the design of emotionally engaging experiences. Additionally, it underscores the potential of language models for scientific inquiries in human-computer interaction.},
	urldate = {2025-03-01},
	booktitle = {2024 {IEEE} {Conference} on {Games} ({CoG})},
	author = {Marincioni, Alessandro and Miltiadous, Myriana and Zacharia, Katerina and Heemskerk, Rick and Doukeris, Georgios and Preuss, Mike and Barbero, Giulio},
	month = aug,
	year = {2024},
	note = {ISSN: 2325-4289},
	keywords = {Collaboration, Emotion recognition, emotions recognition, Focusing, game play, Games, Human computer interaction, Large language models, LLM, NPC, sentiments, video game, Video games},
	pages = {1--6},
}

@misc{kostilainen_next_2024,
	type = {fi={Ylempi} {AMK}-opinnäytetyö{\textbar}sv={Högre} {YH}-examensarbete{\textbar}en={Master}'s thesis{\textbar}},
	title = {Next generation of {NPC} dialogue: creating responsive {NPCs} ({Non}-{Player} {Characters}) with {Retrieval}-{Augmented} {Generation} and real-time player data},
	copyright = {CC BY 4.0},
	shorttitle = {Next generation of {NPC} dialogue},
	url = {http://www.theseus.fi/handle/10024/861943},
	language = {eng},
	urldate = {2025-03-01},
	author = {Kostilainen, Sami},
	year = {2024},
	note = {Accepted: 2024-06-03T08:48:46Z},
}

@article{huang_generating_2024,
        journal = {FIXME},
	title = {Generating dynamic and lifelike {NPC} dialogs in role-playing games using large language model},
	copyright = {fi=Kaikki oikeudet pidätetään.{\textbar}en=All rights reserved.{\textbar}},
	url = {https://lutpub.lut.fi/handle/10024/167809},
	abstract = {Traditional non-player character dialog systems rely on preset lines, which lack realism and dynamics and reduce player immersion. This study aims to explore overcoming these limitations through large language models.

This paper reviews existing non-player character dialogue systems and emphasizes the need to generate dynamic, context-sensitive dialogs. The capabilities of large language models, especially GPT-3 and GPT-4, in generating coherent, contextually relevant content are explored. In this paper, I propose a framework for non-player role dialog based on large language model. The framework includes key components such as intent recognition, memory retrieval, dialog management, and response generation. The framework was tested in practice in the text-based role-playing game Call of Cthulhu. Evaluations from game participants indicated that the dialog based on the large language model was more natural and contextualized, enhancing the overall game experience.

This study demonstrates the feasibility and advantages of integrating large language models into non-player character dialog systems, providing valuable exploration and practice for developing smarter and more responsive game characters. However, there are still challenges in managing long-term memory and context in non-player character dialog systems based on large language models, which require further research.},
	language = {eng},
	urldate = {2025-03-01},
	author = {Huang, Junyang},
	year = {2024},
	note = {Accepted: 2024-06-12T08:14:59Z},
}

@mastersthesis{nagarkar_improving_2024,
	title = {Improving realism and interactivity in games: a study of {AI}-integrated non player characters ({NPCs})},
	copyright = {Open Access},
	shorttitle = {Improving realism and interactivity in games},
	url = {https://upcommons.upc.edu/handle/2117/415440},
	language = {eng},
	urldate = {2025-03-01},
	school = {Universitat Politècnica de Catalunya},
	author = {Nagarkar, Saurabh Bharat},
	month = jul,
	year = {2024},
	note = {Accepted: 2024-10-07T14:25:21Z},
	keywords = {API de GPT-4, Àrees temàtiques de la UPC::Informàtica::Intel·ligència artificial, Artificial intelligence, Artificial Intelligence, Compromís del Jugador, Conversational AI, Disseny de Jocs, Game Design, GPT-4 API, IA Conversacional, Intel·ligència artificial, Intel·ligència Artificial, Interacció en Temps Real, Interactive Storytelling, Narrativa Interactiva, Non-Player Characters (NPCs), Personatges No Jugadors (NPCs), Personatges Virtuals, Player Engagement, Real-Time Interaction, Video games, Video Games, Videojocs, Virtual Characters},
}

@misc{nananukul_what_2024,
	title = {What if {Red} {Can} {Talk}? {Dynamic} {Dialogue} {Generation} {Using} {Large} {Language} {Models}},
	shorttitle = {What if {Red} {Can} {Talk}?},
	url = {http://arxiv.org/abs/2407.20382},
	doi = {10.48550/arXiv.2407.20382},
	abstract = {Role-playing games (RPGs) provide players with a rich, interactive world to explore. Dialogue serves as the primary means of communication between developers and players, manifesting in various forms such as guides, NPC interactions, and storytelling. While most games rely on written scripts to define the main story and character personalities, player immersion can be significantly enhanced through casual interactions between characters. With the advent of large language models (LLMs), we introduce a dialogue filler framework that utilizes LLMs enhanced by knowledge graphs to generate dynamic and contextually appropriate character interactions. We test this framework within the environments of Final Fantasy VII Remake and Pokemon, providing qualitative and quantitative evidence that demonstrates GPT-4's capability to act with defined personalities and generate dialogue. However, some flaws remain, such as GPT-4 being overly positive or more subtle personalities, such as maturity, tend to be of lower quality compared to more overt traits like timidity. This study aims to assist developers in crafting more nuanced filler dialogues, thereby enriching player immersion and enhancing the overall RPG experience.},
	urldate = {2025-03-01},
	publisher = {arXiv},
	author = {Nananukul, Navapat and Wongkamjan, Wichayaporn},
	month = jul,
	year = {2024},
	note = {arXiv:2407.20382 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL Wordplay 2024},
}

@article{gallotta_large_2024,
	title = {Large {Language} {Models} and {Games}: {A} {Survey} and {Roadmap}},
	issn = {2475-1510},
	shorttitle = {Large {Language} {Models} and {Games}},
	url = {https://ieeexplore.ieee.org/abstract/document/10680313},
	doi = {10.1109/TG.2024.3461510},
	abstract = {Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.},
	urldate = {2025-03-01},
	journal = {IEEE Transactions on Games},
	author = {Gallotta, Roberto and Todd, Graham and Zammit, Marvin and Earle, Sam and Liapis, Antonios and Togelius, Julian and Yannakakis, Georgios N.},
	year = {2024},
	note = {Conference Name: IEEE Transactions on Games},
	keywords = {Artificial intelligence, Biological system modeling, Digital games, gameplaying, Games, generative AI, generative text, large language models, Predictive models, procedural content generation, survey, Surveys, Transformers, video games, Video games},
	pages = {1--18},
}

@article{yucong_duan_large_2024,
	title = {"{The} {Large} {Language} {Model} ({LLM}) {Bias} {Evaluation} ({Age} {Bias})" --{DIKWP} {Research} {Group} {International} {Standard} {Evaluation}},
	url = {https://rgdoi.net/10.13140/RG.2.2.26397.12006},
	doi = {10.13140/RG.2.2.26397.12006},
	language = {en},
	urldate = {2025-04-27},
	author = {{Yucong Duan} and {Fuliang Tang} and {Kunguang Wu} and {Zhendong Guo} and {Shuaishuai Huang} and {Yingtian Mei} and {Yuxing Wang} and {Zeyu Yang} and {Shiming Gong}},
	year = {2024},
	note = {Publisher: Unpublished},
}

@article{duan_ranking_nodate,
	title = {"{Ranking} of {Large} {Language} {Model} ({LLM}) {Regional} {Bias}" -- {DIKWP} {Research} {Group} {International} {Standard} {Evaluation}},
	language = {en},
	author = {Duan, Yucong and Tang, Fuliang and Wu, Kunguang and Guo, Zhendong and Huang, Shuaishuai and Mei, Yingtian and Wang, Yuxing and Yang, Zeyu and Gong, Shiming},
}

@article{duan_large_nodate,
	title = {Large {Language} {Model} ({LLM}) {Racial} {Bias} {Evaluation}},
	language = {en},
	author = {Duan, Yucong},
}

@article{llm_security_survey,
	title = {A survey on large language model ({LLM}) security and privacy: {The} {Good}, {The} {Bad}, and {The} {Ugly}},
	volume = {4},
	issn = {2667-2952},
	shorttitle = {A survey on large language model ({LLM}) security and privacy},
	url = {https://www.sciencedirect.com/science/article/pii/S266729522400014X},
	doi = {10.1016/j.hcc.2024.100211},
	abstract = {Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs’ potential to both bolster and jeopardize cybersecurity.},
	number = {2},
	urldate = {2025-04-28},
	journal = {High-Confidence Computing},
	author = {Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
	month = jun,
	year = {2024},
	keywords = {ChatGPT, Large Language Model (LLM), LLM attacks, LLM privacy, LLM security, LLM vulnerabilities},
	pages = {100211},
}

@misc{model_leeching,
	title = {Model {Leeching}: {An} {Extraction} {Attack} {Targeting} {LLMs}},
	shorttitle = {Model {Leeching}},
	url = {http://arxiv.org/abs/2309.10544},
	doi = {10.48550/arXiv.2309.10544},
	abstract = {Model Leeching is a novel extraction attack targeting Large Language Models (LLMs), capable of distilling task-specific knowledge from a target LLM into a reduced parameter model. We demonstrate the effectiveness of our attack by extracting task capability from ChatGPT-3.5-Turbo, achieving 73\% Exact Match (EM) similarity, and SQuAD EM and F1 accuracy scores of 75\% and 87\%, respectively for only \$50 in API cost. We further demonstrate the feasibility of adversarial attack transferability from an extracted model extracted via Model Leeching to perform ML attack staging against a target LLM, resulting in an 11\% increase to attack success rate when applied to ChatGPT-3.5-Turbo.},
	urldate = {2025-04-28},
	publisher = {arXiv},
	author = {Birch, Lewis and Hackett, William and Trawicki, Stefan and Suri, Neeraj and Garraghan, Peter},
	month = sep,
	year = {2023},
	note = {arXiv:2309.10544 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{extraction_attack,
	title = {Training {Data} {Extraction} {Attacks}: {The} {Essential} {Guide} {\textbar} {Nightfall} {AI} {Security} 101},
	shorttitle = {Training {Data} {Extraction} {Attacks}},
	url = {https://www.nightfall.ai/ai-security-101/training-data-extraction-attacks},
	language = {en},
	urldate = {2025-04-28},
}

@misc{prompt_injection,
	title = {Prompt {Injection} attack against {LLM}-integrated {Applications}},
	url = {http://arxiv.org/abs/2306.05499},
	doi = {10.48550/arXiv.2306.05499},
	abstract = {Large Language Models (LLMs), renowned for their superior proficiency in language comprehension and generation, stimulate a vibrant ecosystem of applications around them. However, their extensive assimilation into various services introduces significant security risks. This study deconstructs the complexities and implications of prompt injection attacks on actual LLM-integrated applications. Initially, we conduct an exploratory analysis on ten commercial applications, highlighting the constraints of current attack strategies in practice. Prompted by these limitations, we subsequently formulate HouYi, a novel black-box prompt injection attack technique, which draws inspiration from traditional web injection attacks. HouYi is compartmentalized into three crucial elements: a seamlessly-incorporated pre-constructed prompt, an injection prompt inducing context partition, and a malicious payload designed to fulfill the attack objectives. Leveraging HouYi, we unveil previously unknown and severe attack outcomes, such as unrestricted arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi on 36 actual LLM-integrated applications and discern 31 applications susceptible to prompt injection. 10 vendors have validated our discoveries, including Notion, which has the potential to impact millions of users. Our investigation illuminates both the possible risks of prompt injection attacks and the possible tactics for mitigation.},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and Liu, Yang},
	month = mar,
	year = {2024},
	note = {arXiv:2306.05499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@misc{data_poisoning,
	title = {Is poisoning a real threat to {LLM} alignment? {Maybe} more so than you think},
	shorttitle = {Is poisoning a real threat to {LLM} alignment?},
	url = {http://arxiv.org/abs/2406.12091},
	doi = {10.48550/arXiv.2406.12091},
	abstract = {Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have significantly impacted the alignment of Large Language Models (LLMs). The sensitivity of reinforcement learning algorithms such as Proximal Policy Optimization (PPO) has led to new line work on Direct Policy Optimization (DPO), which treats RLHF in a supervised learning framework. The increased practical use of these RLHF methods warrants an analysis of their vulnerabilities. In this work, we investigate the vulnerabilities of DPO to poisoning attacks under different scenarios and compare the effectiveness of preference poisoning, a first of its kind. We comprehensively analyze DPO's vulnerabilities under different types of attacks, i.e., backdoor and non-backdoor attacks, and different poisoning methods across a wide array of language models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike PPO-based methods, which, when it comes to backdoor attacks, require at least 4{\textbackslash}\% of the data to be poisoned to elicit harmful behavior, we exploit the true vulnerabilities of DPO more simply so we can poison the model with only as much as 0.5{\textbackslash}\% of the data. We further investigate the potential reasons behind the vulnerability and how well this vulnerability translates into backdoor vs non-backdoor attacks.},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Pathmanathan, Pankayaraj and Chakraborty, Souradip and Liu, Xiangyu and Liang, Yongyuan and Huang, Furong},
	month = feb,
	year = {2025},
	note = {arXiv:2406.12091 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{content_moderation,
	title = {Watch {Your} {Language}: {Investigating} {Content} {Moderation} with {Large} {Language} {Models}},
	volume = {18},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {Watch {Your} {Language}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/31358},
	doi = {10.1609/icwsm.v18i1.31358},
	abstract = {Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs can help in content moderation settings. In this work, we evaluate a suite of commodity LLMs on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we instantiate 95 subcommunity specific LLMs by prompting GPT-3.5 with rules from 95 Reddit subcommunities. We find that GPT-3.5 is effective at rule-based moderation for many communities, achieving a median accuracy of 64\% and a median precision of 83\%. For toxicity detection, we evaluate a range of LLMs (GPT-3, GPT-3.5, GPT-4, Gemini Pro, LLAMA 2) and show that LLMs significantly outperform currently widespread toxicity classifiers. However, we also found that increases in model size add only marginal benefit to toxicity detection, suggesting a potential performance plateau for LLMs on toxicity detection tasks. We conclude by outlining avenues for future work in studying LLMs and content moderation.},
	language = {en},
	urldate = {2025-04-29},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Kumar, Deepak and AbuHashem, Yousef Anees and Durumeric, Zakir},
	month = may,
	year = {2024},
	pages = {865--878},
}

@article{academic_integrity,
	title = {Academic integrity considerations of {AI} large language models in the post-pandemic era: {ChatGPT} and beyond},
	volume = {20},
	shorttitle = {Academic integrity considerations of {AI} large language models in the post-pandemic era},
	url = {https://search.informit.org/doi/abs/10.3316/informit.T2024111300009591751711095},
	doi = {10.3316/informit.T2024111300009591751711095},
	abstract = {This paper explores the academic integrity considerations of students' use of Artificial
         Intelligence (AI) tools using Large Language Models (LLMs) such as ChatGPT in formal
         assessments. We examine the evolution of these tools, and highlight the potential
         ways that LLMs can support in the education of students in digital writing and beyond,
         including the teaching of writing and composition, the possibilities of co-creation
         between humans and AI, supporting EFL learners, and improving Automated Writing Evaluations
         (AWE). We describe and demonstrate the potential that these tools have in creating
         original, coherent text that can avoid detection by existing technological methods
         of detection and trained academic staff alike, demonstrating a major academic integrity
         concern related to the use of these tools by students. Analysing the various issues
         related to academic integrity that LLMs raise for both Higher Education Institutions
         (HEIs) and students, we conclude that it is not the student use of any AI tools that
         defines whether plagiarism or a breach of academic integrity has occurred, but whether
         any use is made clear by the student. Deciding whether any particular use of LLMs
         by students can be defined as academic misconduct is determined by the academic integrity
         policies of any given HEI, which must be updated to consider how these tools will
         be used in future educational environments.},
	number = {2},
	urldate = {2025-04-29},
	journal = {Journal of University Teaching and Learning Practice},
	author = {Perkins, Mike},
	month = feb,
	year = {2023},
	note = {Publisher: Open Access Publishing Association (OAPA)},
	keywords = {Artificial intelligence, COVID-19 (Disease)--Social aspects, English language--Study and teaching--Foreign speakers, Plagiarism, Writing--Automation},
	pages = {1--24},
}

@misc{human_narrative,
	title = {Are {Large} {Language} {Models} {Capable} of {Generating} {Human}-{Level} {Narratives}?},
	url = {http://arxiv.org/abs/2407.13248},
	doi = {10.48550/arXiv.2407.13248},
	abstract = {This paper investigates the capability of LLMs in storytelling, focusing on narrative development and plot progression. We introduce a novel computational framework to analyze narratives through three discourse-level aspects: i) story arcs, ii) turning points, and iii) affective dimensions, including arousal and valence. By leveraging expert and automatic annotations, we uncover significant discrepancies between the LLM- and human- written stories. While human-written stories are suspenseful, arousing, and diverse in narrative structures, LLM stories are homogeneously positive and lack tension. Next, we measure narrative reasoning skills as a precursor to generative capacities, concluding that most LLMs fall short of human abilities in discourse understanding. Finally, we show that explicit integration of aforementioned discourse features can enhance storytelling, as is demonstrated by over 40\% improvement in neural storytelling in terms of diversity, suspense, and arousal.},
	urldate = {2025-04-29},
	publisher = {arXiv},
	author = {Tian, Yufei and Huang, Tenghao and Liu, Miri and Jiang, Derek and Spangher, Alexander and Chen, Muhao and May, Jonathan and Peng, Nanyun},
	month = oct,
	year = {2024},
	note = {arXiv:2407.13248 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2024},
}

@inproceedings{academic_integrity2,
	address = {Cham},
	title = {Academic {Integrity} in the {Face} of {Generative} {Language} {Models}},
	isbn = {978-3-031-50215-6},
	doi = {10.1007/978-3-031-50215-6_5},
	abstract = {The increasing sophistication of generative language models and their widespread accessibility to the general public has been a cause of growing concern in academia in recent years. While these AI technologies have the potential to greatly enhance the learning experience and facilitate research, they also pose a significant threat to academic integrity.},
	language = {en},
	booktitle = {Emerging {Technologies} in {Computing}},
	publisher = {Springer Nature Switzerland},
	author = {Meça, Alba and Shkëlzeni, Nirvana},
	editor = {Miraz, Mahdi H. and Southall, Garfield and Ali, Maaruf and Ware, Andrew},
	year = {2024},
	pages = {58--70},
}

@misc{loot_boxes,
	title = {Are {Loot} {Boxes} {Gambling} and {Therefore} {Addictive}?},
	url = {https://kindbridge.com/gaming/are-loot-boxes-gambling-and-therefore-addictive/},
	abstract = {How video game loot boxes may be considered a form of gambling that places vulnerable individuals at risk, from Kindbridge Behavioral Health.},
	language = {en-US},
	urldate = {2025-04-29},
	journal = {Kindbridge Behavioral Health},
	month = jan,
	year = {2024},
}

@misc{mei2025surveycontextengineeringlarge,
      title={A Survey of Context Engineering for Large Language Models}, 
      author={Lingrui Mei and Jiayu Yao and Yuyao Ge and Yiwei Wang and Baolong Bi and Yujun Cai and Jiazhi Liu and Mingyu Li and Zhong-Zhi Li and Duzhen Zhang and Chenlin Zhou and Jiayi Mao and Tianze Xia and Jiafeng Guo and Shenghua Liu},
      year={2025},
      eprint={2507.13334},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.13334}, 
}

@book{pcgbook,
  title={Procedural Content Generation in Games},
  author={Shaker, Noor and Togelius, Julian and Nelson, Mark J.},
  year={2016},
  publisher={Springer},
  doi={10.1007/978-3-319-42716-4}
}

@inproceedings{ding2023evaluationframeworkconversationalagents,
author = {Wang, Clarice and Shi, Yimin and Xiao, Xiaokui},
title = {A Framework for Evaluating AI Agents in Open-Ended Conversations via Scripted Simulation},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737390},
doi = {10.1145/3711896.3737390},
abstract = {Traditional evaluations of conversational AI have primarily focused on ''closed-ended'' interactions, where a human user queries the AI system, such as in customer support. However, many advanced real-world applications-such as job interviews, podcast hosting, and legal or healthcare intake discussions-require ''open-ended'' interactions in which the AI must take initiative by formulating questions to fully understand the human user's story. As AI assumes broader roles that demand greater autonomy, evaluating its performance in open-ended conversations becomes significantly more complex. This paper introduces a novel framework for rigorously assessing AI agents in such open-ended interviews. In this framework, a secondary AI agent (Agent B) simulates a human interviewee by strictly following a structured script that defines topic strengths and weaknesses, along with guidelines for when to hint at or deviate from these topics. Meanwhile, the primary AI agent (Agent A) engages in dynamic questioning to uncover the script's underlying facts and narrative. By comparing the final conversation transcript to the original script, we assess Agent A using metrics such as completeness, consistency, and investigative depth. This approach not only establishes a new benchmark for open-ended conversational skills but also provides insight into how effectively an AI agent can detect and navigate strategic diversions in scripted behavior.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {5810–5818},
numpages = {9},
keywords = {chatbots, large language models, multi-agent systems},
location = {Toronto ON, Canada},
series = {KDD '25}
}
